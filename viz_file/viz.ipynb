{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "090f0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import jsonlines\n",
    "import requests\n",
    "from tqdm import trange\n",
    "import random\n",
    "import json_repair\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9cdf0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reasoning_score(index, pre):\n",
    "    file = open(\"../context_data/a_stars.txt\", \"r\")\n",
    "    a_stars = eval(file.readline())[\"32\"]\n",
    "    file = open(\"../context_data/r_stars.txt\", \"r\")\n",
    "    r_stars = eval(file.readline())[\"32\"]\n",
    "    if a_stars[index] in pre and r_stars[index] in pre:\n",
    "        return 0.5\n",
    "    elif a_stars[index] in pre and r_stars[index] not in pre:\n",
    "        return 1\n",
    "    elif a_stars[index] not in pre and r_stars[index] in pre:\n",
    "        return 0.25\n",
    "    else:\n",
    "        return 0\n",
    "# get formate sky size\n",
    "def get_sky_size(max_context_length, n):\n",
    "    intervel = int(max_context_length / n)\n",
    "    return [i for i in range(intervel, max_context_length + 1, intervel)]\n",
    "\n",
    "# reduce duplicate from the predicted results\n",
    "def reduce_duplicate(predicted, m):\n",
    "    if len(predicted) > m:\n",
    "        predicted = predicted[:m]\n",
    "        predicted = list(set(predicted))\n",
    "    else:\n",
    "        predicted = list(set(predicted))\n",
    "    return predicted\n",
    "\n",
    "# get results from English version of the Counting-Stars\n",
    "def get_data_EN(folder_path, max_context_length, m, n, test_type):\n",
    "    sky_size = get_sky_size(max_context_length, n)\n",
    "    data = []\n",
    "    average_score = 0\n",
    "    indicator = 0\n",
    "    if test_type == \"Acquisition\":\n",
    "        scalar = 0.82\n",
    "    elif test_type == \"Reasoning\":\n",
    "        scalar = 0.815\n",
    "    for item in jsonlines.Reader(folder_path):\n",
    "        if \"```\" in item['answer']:\n",
    "            predicted = json_repair.loads(item['answer'].replace('```','').replace(\"json\",'').strip())['little_penguin']\n",
    "        else:\n",
    "            try:\n",
    "                print(item['answer'])\n",
    "                predicted = json.loads(item['answer'])['little_penguin']\n",
    "            except:\n",
    "                predicted = item['answer']['little_penguin']\n",
    "        predicted = reduce_duplicate(predicted, m)\n",
    "        for i in range(1, m+1):          \n",
    "            counting_times = i\n",
    "            if test_type == \"Acquisition\":\n",
    "                try:\n",
    "                    if item[\"reference_counting_results\"][i-1] in predicted:\n",
    "                        score = 1\n",
    "                    else:\n",
    "                        score = 0\n",
    "                except:\n",
    "                    score = 0\n",
    "            else:\n",
    "                score = get_reasoning_score(counting_times-1, predicted)\n",
    "            average_score += score\n",
    "            data.append({\n",
    "                \"Counting Times\": counting_times,\n",
    "                \"Context Size\": int(item['context_size'] / scalar),\n",
    "                \"Score\": score\n",
    "                })\n",
    "    df = pd.DataFrame(data)\n",
    "    print (df.head())\n",
    "    print (f\"You have {len(df)} rows\")\n",
    "    pivot_table = pd.pivot_table(df, values='Score', index=['Counting Times', 'Context Size'], aggfunc='mean').reset_index()\n",
    "    pivot_table = pivot_table.pivot(index=\"Counting Times\", columns=\"Context Size\", values=\"Score\")\n",
    "    return pivot_table, pivot_table.mean(axis=None).round(3)\n",
    "\n",
    "# get results from Chinese version of the Counting-Stars\n",
    "def get_data_ZH(folder_path, max_context_length, m, n, test_type):\n",
    "    sky_size = get_sky_size(max_context_length, n)\n",
    "    data = []\n",
    "    average_score = 0\n",
    "    indicator = 0\n",
    "    if test_type == \"Acquisition\":\n",
    "        scalar = 0.725\n",
    "    elif test_type == \"Reasoning\":\n",
    "        scalar = 0.72\n",
    "    for item in jsonlines.Reader(folder_path):\n",
    "        if \"```\" in item['answer']:\n",
    "            predicted = json_repair.loads(item['answer'].replace('```','').replace(\"json\",'').strip())['小企鹅']\n",
    "        else:\n",
    "            try:\n",
    "                predicted = json_repair.loads(item['answer'])['小企鹅']   \n",
    "            except:\n",
    "                predicted = item['answer']['小企鹅']\n",
    "        predicted = reduce_duplicate(predicted, m)\n",
    "        for i in range(1, m+1):          \n",
    "            counting_times = i\n",
    "            if test_type == \"Acquisition\":\n",
    "                try:\n",
    "                    if item[\"reference_counting_results\"][i-1] in predicted:\n",
    "                        score = 1\n",
    "                    else:\n",
    "                        score = 0\n",
    "                except:\n",
    "                    score = 0\n",
    "            else:\n",
    "                score = get_reasoning_score(counting_times-1, predicted)\n",
    "            average_score += score\n",
    "            data.append({\n",
    "                \"Counting Times\": counting_times,\n",
    "                \"Context Size\": int(item['context_size'] / scalar),\n",
    "                \"Score\": score\n",
    "                })\n",
    "    df = pd.DataFrame(data)\n",
    "    print (df.head())\n",
    "    print (f\"You have {len(df)} rows\")\n",
    "    pivot_table = pd.pivot_table(df, values='Score', index=['Counting Times', 'Context Size'], aggfunc='mean').reset_index()\n",
    "    pivot_table = pivot_table.pivot(index=\"Counting Times\", columns=\"Context Size\", values=\"Score\")\n",
    "    return pivot_table, pivot_table.mean(axis=None).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0506824c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Counting Times  Sky Size  Score\n",
      "0               1      4000    1.0\n",
      "1               2      4000    1.0\n",
      "2               3      4000    1.0\n",
      "3               4      4000    1.0\n",
      "4               5      4000    1.0\n",
      "You have 1024 rows\n",
      "   Counting Times  Sky Size  Score\n",
      "0               1      4000    1.0\n",
      "1               2      4000    1.0\n",
      "2               3      4000    1.0\n",
      "3               4      4000    1.0\n",
      "4               5      4000    1.0\n",
      "You have 1024 rows\n",
      "   Counting Times  Sky Size  Score\n",
      "0               1      4000    1.0\n",
      "1               2      4000    1.0\n",
      "2               3      4000    1.0\n",
      "3               4      4000    1.0\n",
      "4               5      4000    1.0\n",
      "You have 1024 rows\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "m = 32\n",
    "n = 32\n",
    "max_context_length = 128000\n",
    "\n",
    "testing_type = \"Acquisition\"\n",
    "#testing_type = \"Reasoning\"\n",
    "\n",
    "folder_path_test = open(\"xxx.jsonl\",\"r\")\n",
    "viz_data_gpt, mean_gpt = get_data_ZH(folder_path_test, max_context_length, m, n, testing_type)\n",
    "    \n",
    "folder_path_test = open(\"xxx.jsonl\",\"r\")\n",
    "viz_data_gemini, mean_gemini = get_data_ZH(folder_path_test, max_context_length, m, n, testing_type)\n",
    "    \n",
    "folder_path_test = open(\"xxx.jsonl\",\"r\")\n",
    "viz_data_claude, mean_claude = get_data_ZH(folder_path_test, max_context_length, m, n, testing_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e53cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom colormap. Go to https://coolors.co/ and pick cool colors\n",
    "cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", [\"#184E77\", \"#1E6091\", \"#1A759F\", \"#168AAD\", \"#34A0A4\", \"#52B69A\", \"#76C893\", \"#99D98C\", \"#B5E48C\", \"#D9ED92\"])\n",
    "\n",
    "fig = plt.figure(figsize=(17, 14))\n",
    "ax1 = fig.add_subplot(3, 1, 1)\n",
    "# Create the heatmap with better aesthetics\n",
    "sns.heatmap(\n",
    "    viz_data_gpt4,\n",
    "    #annot=True,\n",
    "    fmt=\"g\",\n",
    "    cmap=cmap,\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={'label': 'Score', \"pad\": 0.02}\n",
    ")\n",
    "\n",
    "labels = [i for i in range(2, m+1, 2)]\n",
    "x = [i-0.5 for i in range(2, m+1, 2)]\n",
    "\n",
    "# More aesthetics\n",
    "plt.title(f'Counting-Stars-({m})-(Multi-evidence {testing_type}): GPT-4 Turbo (Acc: {mean_gpt})', size=11)  # Adds a title\n",
    "plt.xlabel('Context Length', size=11)  # X-axis label\n",
    "plt.ylabel('Counting Times', size=11)  # Y-axis label\n",
    "plt.xticks(rotation=45)  # Rotates the x-axis labels to prevent overlap\n",
    "plt.yticks(x, labels, rotation=45)  # Ensures the y-axis labels are horizontal\n",
    "\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(3, 1, 2)\n",
    "# Create the heatmap with better aesthetics\n",
    "sns.heatmap(\n",
    "    viz_data_claude,\n",
    "    #annot=True,\n",
    "    fmt=\"g\",\n",
    "    cmap=cmap,\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={'label': 'Score', \"pad\": 0.02}\n",
    ")\n",
    "\n",
    "labels = [i for i in range(2, m+1, 2)]\n",
    "x = [i-0.5 for i in range(2, m+1, 2)]\n",
    "\n",
    "# More aesthetics\n",
    "plt.title(f'Counting-Stars-({m})-(Multi-evidence {testing_type}): Claude3 Opus (Acc: {mean_claude})', size=11)  # Adds a title\n",
    "plt.xlabel('Context Length', size=11)  # X-axis label\n",
    "plt.ylabel('Counting Times', size=11)  # Y-axis label\n",
    "plt.xticks(rotation=45)  # Rotates the x-axis labels to prevent overlap\n",
    "plt.yticks(x, labels, rotation=45)  # Ensures the y-axis labels are horizontal\n",
    "\n",
    "\n",
    "ax3 = fig.add_subplot(3, 1, 3)\n",
    "\n",
    "# Create the heatmap with better aesthetics\n",
    "sns.heatmap(\n",
    "    viz_data_gemini,\n",
    "    #annot=True,\n",
    "    fmt=\"g\",\n",
    "    cmap=cmap,\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={'label': 'Score', \"pad\": 0.02}\n",
    ")\n",
    "\n",
    "labels = [i for i in range(2, m+1, 2)]\n",
    "x = [i-0.5 for i in range(2, m+1, 2)]\n",
    "\n",
    "# More aesthetics\n",
    "plt.title(f'Counting-Stars-({m})-(Multi-evidence {testing_type}): Gemini Pro 1.5 (Acc: {mean_gemini})', size=11)  # Adds a title\n",
    "plt.xlabel('Context Length', size=11)  # X-axis label\n",
    "plt.ylabel('Counting Times', size=11)  # Y-axis label\n",
    "plt.xticks(rotation=45)  # Rotates the x-axis labels to prevent overlap\n",
    "plt.yticks(x, labels, rotation=45)  # Ensures the y-axis labels are horizontal\n",
    "\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "plt.savefig(f\"results.pdf\", dpi=2380, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
